{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "524fbae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from sklearn.utils import class_weight\n",
    "from collections import Counter\n",
    "import torch\n",
    "import numpy as np\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43de46ed",
   "metadata": {},
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "626fd3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# === Tokenizer ===\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2c2e2e96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72f099f6bfa649b3adc66a244c63b44a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/389788 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9ef0635735d4dff9bde15a46ee1783c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/48723 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14f8bb6d2d194c32aac8465589875b4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/48724 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# 1) Load the single-file CSV\n",
    "raw_ds = load_dataset(\"csv\", data_files=\"./AI_Human.csv\")[\"train\"]\n",
    "\n",
    "# 2) Split 80 % / 10 % / 10 %\n",
    "tmp        = raw_ds.train_test_split(test_size=0.2, seed=42)\n",
    "train_ds   = tmp[\"train\"]\n",
    "tmp2       = tmp[\"test\"].train_test_split(test_size=0.5, seed=42)\n",
    "val_ds     = tmp2[\"train\"]\n",
    "test_ds    = tmp2[\"test\"]\n",
    "\n",
    "\n",
    "def tokenize_fn(batch):\n",
    "    enc = tokenizer(batch[\"text\"], padding=\"max_length\", truncation=True, max_length=256)\n",
    "    # Force to int for HF Trainer and PyTorch loss:\n",
    "    enc[\"labels\"] = [int(x) for x in batch[\"generated\"]]\n",
    "    return enc\n",
    "\n",
    "token_train = train_ds.map(tokenize_fn, batched=True)\n",
    "token_val   = val_ds.map(tokenize_fn,   batched=True)\n",
    "token_test  = test_ds.map(tokenize_fn,  batched=True)\n",
    "\n",
    "for ds in (token_train, token_val, token_test):\n",
    "    ds.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9e889b4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0) <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "print(token_train[0][\"labels\"], type(token_train[0][\"labels\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589f1b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train distr: Counter({0.0: 244783, 1.0: 145005})\n",
      "Class wts   : tensor([0.7962, 1.3441])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np, torch, collections\n",
    "\n",
    "labels_train   = train_ds[\"generated\"]               \n",
    "weights_np     = class_weight.compute_class_weight(\n",
    "                    \"balanced\",\n",
    "                    classes=np.unique(labels_train),\n",
    "                    y=labels_train,\n",
    "                 )\n",
    "class_weights  = torch.tensor(weights_np, dtype=torch.float)\n",
    "print(\"Train distr:\", collections.Counter(labels_train))\n",
    "print(\"Class wts   :\", class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "15a1ca83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "class WeightedTrainer(Trainer):\n",
    "    def __init__(self, class_weights, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.class_weights = class_weights\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        loss = F.cross_entropy(\n",
    "            logits, labels,\n",
    "            weight=self.class_weights.to(logits.device)\n",
    "        )\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9fab93ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Evaluation metrics ===\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "f1 = evaluate.load(\"f1\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = torch.argmax(torch.tensor(logits), dim=1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy.compute(predictions=preds, references=labels)[\"accuracy\"],\n",
    "        \"f1\": f1.compute(predictions=preds, references=labels)[\"f1\"]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d5816ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_init():\n",
    "    return AutoModelForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "834049cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hp_space(trial):\n",
    "    return {\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-5, 5e-5, log=True),\n",
    "        \"weight_decay\": trial.suggest_float(\"weight_decay\", 0.0, 0.3),\n",
    "        \"lora_dropout\": trial.suggest_float(\"lora_dropout\", 0.0, 0.3),\n",
    "        \"per_device_train_batch_size\": 8,\n",
    "        \"num_train_epochs\": 1,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "42fc525b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"no\",\n",
    "    learning_rate=2e-5,  # gets overridden by Optuna\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=1,\n",
    "    load_best_model_at_end=False,\n",
    "    report_to=\"none\",\n",
    "    fp16=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "87e0fd4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dayso\\AppData\\Local\\Temp\\ipykernel_6420\\4118468332.py:4: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(*args, **kwargs)\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "[I 2025-08-01 00:20:32,210] A new study created in memory with name: no-name-2103c78a-d66d-499e-84cd-2356ca70cff9\n",
      "Trying to set lora_dropout in the hyperparameter search but there is no corresponding field in `TrainingArguments`.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='48724' max='48724' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [48724/48724 1:19:36, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.003700</td>\n",
       "      <td>0.001693</td>\n",
       "      <td>0.999631</td>\n",
       "      <td>0.999504</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-01 01:40:09,325] Trial 0 finished with value: 1.9991349153198215 and parameters: {'learning_rate': 1.06616603285535e-05, 'weight_decay': 0.23300051397406885, 'lora_dropout': 0.16514461747589287}. Best is trial 0 with value: 1.9991349153198215.\n",
      "Trying to set lora_dropout in the hyperparameter search but there is no corresponding field in `TrainingArguments`.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='48724' max='48724' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [48724/48724 1:23:02, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000928</td>\n",
       "      <td>0.999836</td>\n",
       "      <td>0.999780</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-01 03:03:23,254] Trial 1 finished with value: 1.999615457244391 and parameters: {'learning_rate': 1.6035947409854068e-05, 'weight_decay': 0.18655360030594448, 'lora_dropout': 0.16533904614382647}. Best is trial 1 with value: 1.999615457244391.\n",
      "Trying to set lora_dropout in the hyperparameter search but there is no corresponding field in `TrainingArguments`.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='48724' max='48724' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [48724/48724 1:23:06, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001459</td>\n",
       "      <td>0.999733</td>\n",
       "      <td>0.999642</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-01 04:26:30,635] Trial 2 finished with value: 1.9993751673277915 and parameters: {'learning_rate': 2.0925258326344382e-05, 'weight_decay': 0.20066625242766514, 'lora_dropout': 0.05811072515729007}. Best is trial 1 with value: 1.999615457244391.\n",
      "Trying to set lora_dropout in the hyperparameter search but there is no corresponding field in `TrainingArguments`.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='48724' max='48724' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [48724/48724 10:29:31, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.692300</td>\n",
       "      <td>0.693094</td>\n",
       "      <td>0.627507</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-01 14:56:13,902] Trial 3 finished with value: 0.6275065164296123 and parameters: {'learning_rate': 4.843665732392676e-05, 'weight_decay': 0.24883025013828303, 'lora_dropout': 0.1506740902168985}. Best is trial 1 with value: 1.999615457244391.\n",
      "Trying to set lora_dropout in the hyperparameter search but there is no corresponding field in `TrainingArguments`.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='48724' max='48724' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [48724/48724 1:27:33, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004126</td>\n",
       "      <td>0.999261</td>\n",
       "      <td>0.999009</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-01 16:23:48,809] Trial 4 finished with value: 1.998270267190827 and parameters: {'learning_rate': 1.5729027809573185e-05, 'weight_decay': 0.004669289081455685, 'lora_dropout': 0.0773765920389505}. Best is trial 1 with value: 1.999615457244391.\n",
      "Trying to set lora_dropout in the hyperparameter search but there is no corresponding field in `TrainingArguments`.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='48724' max='48724' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [48724/48724 1:16:12, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.692500</td>\n",
       "      <td>0.693029</td>\n",
       "      <td>0.627507</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-01 17:40:11,895] Trial 5 pruned. \n",
      "Trying to set lora_dropout in the hyperparameter search but there is no corresponding field in `TrainingArguments`.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='48724' max='48724' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [48724/48724 1:20:49, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.002100</td>\n",
       "      <td>0.003649</td>\n",
       "      <td>0.999425</td>\n",
       "      <td>0.999229</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-01 19:01:02,590] Trial 6 pruned. \n",
      "Trying to set lora_dropout in the hyperparameter search but there is no corresponding field in `TrainingArguments`.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='48724' max='48724' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [48724/48724 1:24:00, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001331</td>\n",
       "      <td>0.999774</td>\n",
       "      <td>0.999697</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-01 20:25:14,274] Trial 7 finished with value: 1.9994712787445188 and parameters: {'learning_rate': 1.50078980854786e-05, 'weight_decay': 0.037100892697281874, 'lora_dropout': 0.0997016452643168}. Best is trial 1 with value: 1.999615457244391.\n",
      "Trying to set lora_dropout in the hyperparameter search but there is no corresponding field in `TrainingArguments`.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='48724' max='48724' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [48724/48724 1:22:50, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.008037</td>\n",
       "      <td>0.998830</td>\n",
       "      <td>0.998431</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-01 21:48:05,601] Trial 8 pruned. \n",
      "Trying to set lora_dropout in the hyperparameter search but there is no corresponding field in `TrainingArguments`.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='48724' max='48724' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [48724/48724 1:35:23, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004770</td>\n",
       "      <td>0.999159</td>\n",
       "      <td>0.998872</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-01 23:23:40,088] Trial 9 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'learning_rate': 1.6035947409854068e-05, 'weight_decay': 0.18655360030594448, 'lora_dropout': 0.16533904614382647}\n"
     ]
    }
   ],
   "source": [
    "trainer = WeightedTrainer(                      # ‚úèÔ∏è CHANGE\n",
    "    class_weights=class_weights,                # üîß NEW\n",
    "    model_init=model_init,\n",
    "    args=training_args,\n",
    "    train_dataset=token_train,                  # ‚úèÔ∏è CHANGE\n",
    "    eval_dataset=token_val,                     # ‚úèÔ∏è CHANGE\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "best_run = trainer.hyperparameter_search(\n",
    "    direction=\"maximize\",\n",
    "    n_trials=10,\n",
    "    hp_space=hp_space,\n",
    "    backend=\"optuna\"\n",
    ")\n",
    "\n",
    "print(\"Best hyperparameters:\", best_run.hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca625d80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.) <class 'torch.Tensor'> torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "print(token_train[0][\"labels\"], type(token_train[0][\"labels\"]), token_train[0][\"labels\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "63994e2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0) <class 'torch.Tensor'>\n",
      "tensor([0, 1, 0, 0, 0, 0, 0, 1])\n",
      "torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "print(token_train[0][\"labels\"], type(token_train[0][\"labels\"]))  # Should be int (not float), e.g. tensor(0)\n",
    "print(token_train[:8][\"labels\"])  # Should be a 1D tensor/list, e.g. tensor([0, 1, 0, ...])\n",
    "print(token_train[:8][\"labels\"].shape)  # Should be torch.Size([8])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_CV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
